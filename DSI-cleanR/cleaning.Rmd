---
title: "Data cleaning in R"
author: "Simon Knight, sjgknight@gmail.com, modified to learnr in 2021"
date: "`r Sys.Date()`"
output: learnr::tutorial
runtime: shiny_prerendered
description: "Welcome to this learnr tutorial!"
---

# Data cleaning in R and OpenRefine

If you're here, you've probably already setup a new project and created this notebook file - but if not, go to the square on the top right of RStudio to create a new project.

******

This learnr activity steps through a set of tasks adapted from an OpenRefine tutorial (see https://goo.gl/mQC7oe). My advice is:

* **If you care more about techniques for data cleaning and wrangling than coding** use the pure OpenRefine version. OpenRefine is a fantastic graphical interface for these tasks, and I think incredibly valuable for many organisations.
* **If you want to learn some R alongside cleaning and wrangling** this is a good version for you! 

^[This resource is adapted from:
1.	http://enipedia.tudelft.nl/wiki/OpenRefine_Tutorial#Clean_up_country_names (under a CC-By-SA 3.0 license) C.B.Davis, A. Chmieliauskas, G.P.J. Dijkema, I. Nikolic (2015), Enipedia, http://enipedia.tudelft.nl, Energy & Industry group, Faculty of Technology, Policy and Management, TU Delft, Delft, The Netherlands. 
2.	And http://schoolofdata.org/handbook/recipes/cleaning-data-with-refine/ 3. Knight, S., (2016). Cleaning Data with Refine, at see https://goo.gl/mQC7oe]

******

# Cleaning Data in R: A learnR tutorial

## Setting up in R
First we're going to load our data

```{r setup}
library(tidyverse)

unis <- read.csv("https://goo.gl/EJxa20", header=T, stringsAsFactors=F)
```

## Exploring Data

We can use base`base R` to show parts of our data like this
```{r}
#We use 'head' to show the top
head(unis)

#Tail to show the bottom
tail(unis)

#And we can look at specific parts of the data, e.g.
unis[1:100,]

colnames(unis)

#we can also sort it
#unis <- unis[with(unis, order(country, university)), ]  #Base R
unis <- arrange(unis, country, university) #the tidy approach

head(unis)
```

============================================================

## NAs, encoding, data types, etc. in the endowment 

### Dealing with NAs

```{r}
#here it's useful to do two things, first learn about replacement, e.g.:
#unis$established[unis$established==""] <- NA Base R
#str_replace from the stringr package is also very useful


#and to explore your values
table(unis$established)

#hm...there are a lot of those, how about
table(unis[c("established")][is.character(unis$established),])


```

### And other encoding issues

```{r}

unis %>% mutate(asc = printable_ascii(endowment)) %>% filter(asc == FALSE) %>% select(endowment) %>% unique

unis$endowment <- str_replace(unis$endowment, "ï¿½", "")

```

Let's see if we can visualise this now using `summarise` from base r (which gives a more informative error message...) `r summarise(unis$endowment)`

### Oh no! The endowment is chracater not $$$$

```{r}


```



```{r}

```




## Slicing, filtering, or faceting data by a value

```{r}
#in R this is probably most easily achieved in base R using the subset function
#see http://www.statmethods.net/management/subset.html 
#subset(unis, university == "Cardiff University" | university == "Acadia University", select = c("university", "country", "established"))
#this is the same thing, it's considered superior for reasons that don't matter here
#unis[c("university","country","established")][unis$university == "Cardiff University" | unis$university == "Acadia University", ]
#subset(unis, university == "Cambridge", select = c("university", "country", "established"))
#unis[grep("Cambridge", unis$university), ]
#subset(unis, grepl("Cambridge", unis$university))
##########################################################
##########################################################
# We're going to do this using tidyverse functions

unis %>% filter(university == "Cardiff University" | university == "Acadia University") %>%
  select(university,country,established)

#but often, we'll want our filter to be more subtle than this, consider for example
unis %>% filter(university == "Cambridge") %>%
  select(university, country, established)
#but...we know that institution exists

unis %>% filter(str_detect(university,"Cambridge")) %>%
  select(university,country,established)

#note this isn't quite right ("Cambridge College" is a rather different place), we could use different ways to filter this, here's one involving negation using !
unis %>% filter(str_detect(university,"Cambridge") & !str_detect(university,"College")) %>%
  select(university,country,established)



```

```{r}
#ok so it looks like there might be two options here.
#either, we use a fuzzy matching (e.g. using agrep) to identify strings that look similar, possibly using a ground truth (e.g. a reference list of university names) to match these against
#in OpenRefine this is called reconciliation http://freeyourmetadata.org/reconciliation/ 


#or - a slower mechanism - we use entity recognition
#it's worth noting that in this discussion of a similar problem, someone recommends using openrefine! https://stackoverflow.com/questions/6683380/techniques-for-finding-near-duplicate-records 
#https://www.r-bloggers.com/merging-data-sets-based-on-partially-matched-data-elements/ and this great blog post discusses implementing some of the openrefine approaches in R (although, only partially, and not all of them - i.e., openrefine is really really powerful!)

#using entity  recognition!
#https://rpubs.com/lmullen/nlp-chapter
```

```{r}
library(openNLP)
library(NLP)
entities <- function(doc, kind) {
  s <- doc$content
  a <- annotations(doc)[[1]]
  if(hasArg(kind)) {
    k <- sapply(a$features, `[[`, "kind")
    s[a[k == kind]]
  } else {
    s[a[a$type == "entity"]]
  }
}

#text <- c("the year 2015 is now")
word_ann <- Maxent_Word_Token_Annotator()
sent_ann <- Maxent_Sent_Token_Annotator()
location_ann <- Maxent_Entity_Annotator(kind = "location")
organization_ann <- Maxent_Entity_Annotator(kind = "organization")
money_ann <- Maxent_Entity_Annotator(kind = "money")
date_ann <- Maxent_Entity_Annotator(kind = "date")

pipeline <- list(sent_ann,
                 word_ann,
                 location_ann,
                 organization_ann,
                 money_ann,
                 date_ann)
```

Note we only need to do this for unique values, given it's very timeconsuming it's well worth subsetting and rejoining
```{r}
estab <- as.data.frame(unique(unis$established))

estab$established_ann <- lapply(estab$established, function(x) annotate(x, list(sent_ann,word_ann,date_ann)))

estab$established_ann_plain <- apply(estab[c("unique(estab$established)","established_ann")], 1, function(x) AnnotatedPlainTextDocument(x[[1]], x[[2]]))

estab$established_date_extract <- lapply(estab$established_ann_plain, function(x) (x)[[1]][[1]])

estab$established_date_extract <- unlist(estab$established_date_extract)
#a tonne of cleaning is likely here, but at least its of date values!

unis <- merge(unis, estab[c("unique(unis$established)","established_date_extract")], by.x="established", by.y="unique(unis$established)" )

#let's just save this, we don't want to need  to redo the labelling! 
#we need to select columns if you want to save as csv because you can't save list 

save(unis,file="unis.Rda")
```


```{r}
library(stringr) #for some cleaning
unis$university <- str_trim(unis$university,side = "both")
#converting things like this is surprisingly hard if you don't know what to search for...: ("Universidad Ju%C3%A1rez Aut%C3%B3noma de Tabasco")

org <- as.data.frame(unique(unis$university))

org$uni_org <- lapply(org$`unique(unis$university)`, function(x) annotate(x, list(sent_ann,word_ann,organization_ann)))

org$uni_org_plain <- apply(org[c("unique(unis$university)","uni_org")], 1, function(x) AnnotatedPlainTextDocument(x[[1]], x[[2]]))


org$uni_org_extract <- lapply(org$uni_org_plain, function(x) (x)[[1]][[1]])

org$uni_org_extract <- unlist(org$uni_org_extract)

View(org[grep("Cambridge", org$`unique(unis$university)`), ])

unis <- merge(unis, org[c("unique(unis$university)", "uni_org_extract")], by.x = "university", by.y="unique(unis$university)")

save(unis,file="unis.Rda")

```

```{r}
#how do we convert words to numbers, given messy data?
endow <- as.data.frame(unique(unis$endowment))

endow$endowment_mon <- lapply(endow$`unique(unis$endowment)`, function(x) annotate(x, list(sent_ann,word_ann,money_ann)))

endow$endowment_mon_plain <- apply(endow[c("unique(unis$endowment)","endowment_mon")], 1, function(x) AnnotatedPlainTextDocument(x[[1]], x[[2]]))

endow$endowment_mon_extract <- lapply(endow$endowment_mon_plain, function(x) (x)[[1]][[1]])

endow$endowment_mon_extract <- unlist(endow$endowment_mon_extract)  #I haven't checked these at all, do they look about right?

unis <- merge(unis, endow[c("unique(unis$endowment)", "endowment_mon_extract")], by.x="endowment", by.y="unique(unis$endowment)")

save(unis,file="unis.Rda")

#load("unis.Rda")

#write.csv(unis,"unis.csv") #let's just save this, we don't want to need  to redo the 
```




```{r}
#extract locations, but also merge near duplicates
#devtools::install_github("ropensci/monkeylearn")

```

Step 8: Deduplicate entries ([near] duplicate rows)
There are a lot of (nearly) duplicate rows in the data, this can happen for various reasons including human error and because multiple historic values are stored. We want to keep just one copy.
To do this (based on documentation here), click on the column with the university names, and then click on "Sort". Once you do this, you will notice that there is a new "Sort" menu at the top. Click on this and select "Reorder rows permanently". This may take a while as it renumbers the rows in which the entries appear. 

Then on the column with university names, Edit cells -> Blank down 

Then on the same column, Facet -> Customized facets -> Facet by blank 
Now we want to remove all the blank rows, so select true, then on the "All" column on the left, Edit rows -> Remove all matching rows, like you have done when working with the numStudents and endowment columns. 
Once you remove all the facets, and you now have a (mostly) cleaned data set. 


```{r}
#rather than do it this way, we'd probably use the entity recognition approach

```


Step 9: Exploring the data with scatter plots
Click on the "endowment" column, Facet -> Scatterplot facet. 
This shows the relationships between all of the numeric values in each of the columns. Click on "log" to get a better view. 

Click on the plot for endowment vs. numStudents. You can now drag select a portion of the plot, and then see the rows corresponding to that selection. 

```{r}

```

Step 10: Geocoding names and addresses
This next part shows (based on documentation here) how to go from a description of a place (i.e. the name of a university) to values for its (likely) geographic coordinates. Behind the scenes, this uses Google Maps to figure out what is the most likely location you are asking for. 
To learn how to do this, you don't need to do process the whole data set. This can take a while, and Google limits you to 2000 requests per day. It's better to just select around 10 rows and verify that it works. 
An easy way to get a limited set of rows is by using a numeric log facet of the number of students, so use Facet -> Customized facets -> Numeric log facet 
 
Use this facet to make a selection of around ten rows, and then check the matching rows number to verify that you have a reasonable selection size: 


Now the fun begins and we want to do Edit column -> Add column by fetching URLs. In other words, the values of the cells in the new column are based on data that is retrieved from the Internet. 
Enter in the expression below, and you should see a list of URLs with the names of the universities at the end of the URLs. Specify a new column name such as "geocodingResponse", and set the throttle delay to around 500 milliseconds. 
"http://maps.google.com/maps/api/geocode/json?sensor=false&address=" + escape(value, "url")


You should get a bunch of data back. To convert this into a more readable format, you need to click on the geocodingResponse column, and then on Edit column -> Add column based on this column. Enter in the expression below 
with(value.parseJson().results[0].geometry.location, pair, pair.lat +", " + pair.lng)
Now you have a single column with coordinates. You can split this into columns for latitude and longitude by selecting Edit Column -> Split into several columns and specifying a separator of ",". These columns can then be renamed using Edit Column -> Rename this column. 
```{r}
#there are a few ways we can do this in R
#the most obvious one is using googlemaps api geocoding https://cengel.github.io/rspatial/5_Geocoding.nb.html#issues - but it limits us to 2500 queries a day.  to use that 
#library(ggmap)
#geocode(unis$uni_org_extract[[1]])  (,etc.) to get latlong

#I think it should also be possible to use WikiData (possibly DBPedia) to access the organisation coordinates. Likely more missing data(?) but that can always be filled with google if necessary without first using the 2500 a day limit....of course if you subset to unique unis there are a lot fewer! 
library(WikidataR)

wiki_unis <- 

#ok so first we have to find the Wikidata entity code for all of the universities
unis$wikidata <- NA

wkd <- as.data.frame(unique(unis$uni_org_extract))
wkd$wikidata <- NA
wkd$wikidata <- lapply(wkd[,1], function(x) find_item((x), language = "en", limit = 1))[[1]]$id #FALSE POSITIVES!!  This is pretty slow
#wkd$wikidata <- lapply(wkd$wikidata, function(x) ifelse(length(x) > 0, unlist(x[[1]][[2]]), ""))

wkd$lat <- NA
wkd$long <- NA

#get_geo_entity("Q35794", language = "en", radius = NULL)[[c(3,4)]]
x <-  (lapply(wkd[,2], function(x) {
  if(x != "") {
    unlist(get_geo_entity(x, language = "en", radius = NULL)[c(3:4)])
    } else {
      ""}
}))

wkd$lat <- unlist(lapply(x, function(y) ifelse(length(y) > 0, unlist(y)[1], "")))
wkd$long <- unlist(lapply(x, function(y) ifelse(length(y) > 0, unlist(y)[2], "")))

rm(x)
save(wkd,file="unis_location.Rda")

load("unis_location.Rda")

#ok, now we can map this. Again, we can do that in OpenRefine on google fusion tables really easily. To do it in R requires a bit of leg work, and there are various options depending on what we want to visualise (e.g., do we want to map universities to points, or to do a heatmap of endowment sizes, or...)

```

Step 11: Export Data
The data can be exported to formats such as Excel. If you read this into tools such as SPSS and notice that the last column is missing, then open the file up in Excel, re-save it, and try to open it up again in SPSS. 
```{r}

```



```{r}
library(WikipediaR)

pages_in_category(language = "en", project = "wikipedia", categories = "")

```

